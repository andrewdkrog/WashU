{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import re\n",
    "\n",
    "def scrape():\n",
    "    \n",
    "    # Set up splinter browser\n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    # Create dictionary to store scraped Mars data\n",
    "    mars_data = {}\n",
    "\n",
    "    # NASA Mars news url to scrape\n",
    "    url_mars_news = \"https://mars.nasa.gov/news/\"\n",
    "\n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url_mars_news)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "    # Find content titles, store first title\n",
    "    titles = soup.find_all('div', class_=\"content_title\")\n",
    "    news_title = titles[0].text.replace('\\n', '')\n",
    "\n",
    "    # Find content description, save first one\n",
    "    teasers = soup.find_all('div', class_=\"rollover_description_inner\")\n",
    "    news_p = teasers[0].text.replace('\\n', '')\n",
    "\n",
    "    mars_data[\"news_title\"] = news_title\n",
    "    mars_data[\"summary\"] = news_p\n",
    "\n",
    "    # Mars image url to scrape\n",
    "    image_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(image_url)\n",
    "\n",
    "    # Save site's html as variable, use beautiful soup to parse\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    # Find html with article tag using lambda function to search for style that begins with 'background-image'\n",
    "    extracted_html = soup.find(\"article\", {\"style\" : lambda L: L and L.startswith('background-image')})\n",
    "\n",
    "    # Store the style as variable\n",
    "    extracted_style = extracted_html.attrs['style']\n",
    "\n",
    "    beg_url = 'https://www.jpl.nasa.gov'\n",
    "\n",
    "    # Use regular expressions package to search for url\n",
    "\n",
    "    try:\n",
    "        found = re.search('url(.+?);', extracted_style).group(1)\n",
    "    except AttributeError:\n",
    "        found = ''\n",
    "\n",
    "    # Create featured image url by combining two strings\n",
    "    featured_image_url = beg_url + found[2:-2]\n",
    "\n",
    "    mars_data[\"featured_image_url\"] = featured_image_url\n",
    "\n",
    "\n",
    "    # url of Mars twitter page\n",
    "    twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(twitter_url)\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "    # Save latest tweet text as string\n",
    "    mars_weather = soup.find('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text\n",
    "\n",
    "    mars_data[\"mars_weather\"] = mars_weather\n",
    "\n",
    "    # Mars facts url\n",
    "    facts_url = 'http://space-facts.com/mars/'\n",
    "\n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(facts_url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "    # Use pandas to read the table from html\n",
    "    mars_facts = pd.read_html(facts_url)\n",
    "\n",
    "    # Subset list to get table\n",
    "    mars_df = mars_facts[0]\n",
    "\n",
    "    # Name table columns\n",
    "    mars_df.columns = ['Attribute', 'Value']\n",
    "\n",
    "    # Convert table to html\n",
    "    mars_html_table = mars_df.to_html()\n",
    "\n",
    "    # Remove all html code for new lines\n",
    "    mars_html_table.replace('\\n', '')\n",
    "\n",
    "    # Save as html table\n",
    "    mars_info = mars_df.to_html('mars_html_table.html')\n",
    "\n",
    "    mars_data[\"mars_table\"] = mars_info\n",
    "\n",
    "    url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "    mars_hemis = []\n",
    "\n",
    "    for i in range (4):\n",
    "        images = browser.find_by_tag('h3')\n",
    "        images[i].click()\n",
    "        html = browser.html\n",
    "        soup = bs(html, 'html.parser')\n",
    "        partial = soup.find(\"img\", class_=\"wide-image\")[\"src\"]\n",
    "        img_title = soup.find(\"h2\",class_=\"title\").text\n",
    "        img_url = 'https://astrogeology.usgs.gov'+ partial\n",
    "        dictionary={\"title\":img_title,\"img_url\":img_url}\n",
    "        mars_hemis.append(dictionary)\n",
    "        browser.back()\n",
    "\n",
    "    mars_data['mars_hemis'] = mars_hemis\n",
    "\n",
    "\n",
    "    # Return the dictionary\n",
    "    return mars_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'news_title': 'Opportunity Hunkers Down During Dust Storm', 'summary': \"It's the beginning of the end for the planet-encircling dust storm on Mars. But it could still be weeks, or even months, before skies are clear enough for NASA's Opportunity rover to recharge its batteries and phone home. \", 'featured_image_url': 'https://www.jpl.nasa.gov/spaceimages/images/wallpaper/PIA18904-1920x1200.jpg', 'mars_weather': 'Jose Morales captured Mars from Chicago last night. 15000 frames for this Mars tonight.  The South Pole, Syrtis Major Planum, and Hellas Planitia are visible.pic.twitter.com/cFkgmdoHDV', 'mars_table': None, 'mars_hemis': [{'title': 'Cerberus Hemisphere Enhanced', 'img_url': 'https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg'}, {'title': 'Schiaparelli Hemisphere Enhanced', 'img_url': 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg'}, {'title': 'Syrtis Major Hemisphere Enhanced', 'img_url': 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg'}, {'title': 'Valles Marineris Hemisphere Enhanced', 'img_url': 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg'}]}\n"
     ]
    }
   ],
   "source": [
    "data = scrape()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Collection' object is not callable. If you meant to call the 'update' method on a 'Database' object it is failing because no such method exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8d2af7da3c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mupsert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pymongo\\collection.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3315\u001b[0m                             \u001b[1;34m\"object it is failing because no such method \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3316\u001b[0m                             \u001b[1;34m\"exists.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3317\u001b[1;33m                             self.__name)\n\u001b[0m\u001b[0;32m   3318\u001b[0m         raise TypeError(\"'Collection' object is not callable. If you meant to \"\n\u001b[0;32m   3319\u001b[0m                         \u001b[1;34m\"call the '%s' method on a 'Collection' object it is \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Collection' object is not callable. If you meant to call the 'update' method on a 'Database' object it is failing because no such method exists."
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient \n",
    "  \n",
    "conn = MongoClient() \n",
    "db = conn.database \n",
    "  \n",
    "# Created or Switched to collection names: my_gfg_collection \n",
    "collection = db.mars\n",
    "\n",
    "mars.update(\n",
    "    {},\n",
    "    data,\n",
    "    upsert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
